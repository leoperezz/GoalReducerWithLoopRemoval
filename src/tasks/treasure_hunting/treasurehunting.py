""" 
Treasure hunting task.

Map:

Cow Field (0) ---- Scarecrow (1)
|                             |
Farm House (3) --- Axe Stump (2)

The state and goal representation have embeddings that
are generated by OpenAI text-embedding-ada-002 model.

"""
import copy
from typing import Optional
import gymnasium as gym
from gymnasium import spaces
import json
import numpy as np
from pathlib import Path
import random


class TreasureHuntEnv(gym.Env):
    metadata = {"render_modes": ["human", "vec"], "render_fps": 4}

    def __init__(self, seed: Optional[int] = None, render_mode: str = "vec"):
        assert render_mode in ("vec", "human")
        self.render_mode = render_mode

        self.max_steps = 2

        if seed is not None:
            print(f"set seed to {seed}", end="\r")
            random.seed(seed)
            np.random.seed(seed)

        self.loc_neighbors = {
            0: (3, 1),
            1: (0, 2),
            2: (1, 3),
            3: (2, 0),
        }
        self.loc_name_mapping = {
            0: "Cows",
            1: "Scrow",
            2: "Stump",
            3: "House",
        }
        config_path = Path(__file__).parent / "task_config.json"
        self.stimuli_info = json.load(open(config_path, "r"))
        self.ach_goal_info = {}

        maxv = -np.inf
        minv = np.inf

        self.akcs = []
        for akc, ackv in self.stimuli_info["goal_info"].items():
            a, k, c = [int(s) for s in akc.split("-")]
            akc = (a, k, c)
            self.akcs.append(akc)

        for kc, ackv in self.stimuli_info["ach_goal_info"].items():
            k, c = [int(s) for s in kc.split("-")]
            kc = (k, c)
            self.ach_goal_info[kc] = ackv
            self.ach_goal_info[kc]["emb"] = np.array(
                self.ach_goal_info[kc]["emb"],
            )
            maxv = max(maxv, self.ach_goal_info[kc]["emb"].max())
            minv = min(minv, self.ach_goal_info[kc]["emb"].min())

        self.obs_info = {}
        for kys in self.stimuli_info["obs_info"].keys():
            loc = int(kys)
            self.obs_info[loc] = self.stimuli_info["obs_info"][kys]
            self.obs_info[loc]["emb"] = np.array(self.obs_info[loc]["emb"])
            maxv = max(maxv, self.obs_info[loc]["emb"].max())
            minv = min(minv, self.obs_info[loc]["emb"].min())

        shape = self.obs_info[loc]["emb"].shape

        self.action_space = spaces.Discrete(4)
        self.observation_space = spaces.Dict(
            {
                "observation": spaces.Box(minv, maxv, shape=shape,
                                          dtype=float),
                "desired_goal": spaces.Box(minv, maxv, shape=shape,
                                           dtype=float),
                "achieved_goal": spaces.Box(minv, maxv, shape=shape,
                                            dtype=float),
            }
        )

        self._action_to_direction = {
            0: "A",
            1: "W",
            2: "D",
            3: "S",
        }
        self.dir2act = {v: k for k, v in self._action_to_direction.items()}
        self.optimal_1step_acts = {
            # s, g -> a
            (0, 1): 'D',
            (0, 3): 'S',

            (1, 0): 'A',
            (1, 2): 'S',

            (2, 3): 'A',
            (2, 1): 'W',

            (3, 0): 'W',
            (3, 2): 'D',
        }

        self.reset()

    def _get_obs(self):
        a = self.current_state_info["agent"]
        akc = self.current_state_info["akc"]
        kc = akc[1:]
        achieved_kc = (a, a)
        return {
            "desired_goal": self.ach_goal_info[kc]["emb"],
            "observation": self.obs_info[a]["emb"],
            "achieved_goal": self.ach_goal_info[achieved_kc]["emb"],
        }

    def render(self, direction=None):
        a = self.current_state_info["agent"]
        akc = self.current_state_info["akc"]
        t_info = "=" * 10 + "t=" + str(self.steps) + "=" * 10
        if direction is not None:
            dir_info = f"You moved {direction}\n"
        else:
            dir_info = ""
        kc = akc[1:]
        print(f'{t_info}\n{dir_info}Goal:\n{self.ach_goal_info[kc]["verbal"]}\n\n'
              f'Obs:\n{self.obs_info[a]["verbal"]}\n')

    def reset(self, akc=None, seed=None, options=None):
        if seed is not None:
            print(f"set seed to {seed}", end="\r")
            random.seed(seed)
            np.random.seed(seed)

        if akc is None:
            akc = self.akcs[np.random.choice(len(self.akcs), 1)[0]]

        assert akc in self.akcs
        a, k, c = akc
        self.current_state_info = {
            "prev_agent_pos": a,
            "agent": a,
            "key": k,
            "chest": c,
            "got_key": False,
            "akc": akc,
        }
        self.steps = 0
        if self.current_state_info["agent"] == self.current_state_info["key"]:
            self.current_state_info["got_key"] = True

        if self.render_mode == "human":
            self.render()

        info = self.current_state_info
        return self._get_obs(), info

    def step(self, action):
        self.steps += 1
        direction = self._action_to_direction[action]
        a_loc = self.current_state_info["agent"]
        self.current_state_info["prev_agent_pos"] = copy.deepcopy(a_loc)
        if a_loc == 0:
            if direction == "D":
                a_loc = 1
            elif direction == "S":
                a_loc = 3
        elif a_loc == 1:
            if direction == "A":
                a_loc = 0
            elif direction == "S":
                a_loc = 2
        elif a_loc == 2:
            if direction == "A":
                a_loc = 3
            elif direction == "W":
                a_loc = 1
        elif a_loc == 3:
            if direction == "W":
                a_loc = 0
            elif direction == "D":
                a_loc = 2
        self.current_state_info["agent"] = a_loc

        reward = 0
        if self.current_state_info["got_key"] is False:
            if self.current_state_info["agent"] == self.current_state_info["key"]:
                self.current_state_info["got_key"] = True
                reward += 1
        else:
            if self.steps == 1:
                reward += 1

        terminated = False
        if self.steps >= self.max_steps:
            terminated = True

        observation = self._get_obs()
        if self.current_state_info["got_key"] is True:
            if self.current_state_info["agent"] == self.current_state_info["chest"]:
                reward += 1
                observation['achieved_goal'] = observation['desired_goal']
                terminated = True

        # if reward == 2:

        info = self.current_state_info

        if self.render_mode == "human":
            self.render(direction)

        return observation, reward, terminated, False, info


if __name__ == "__main__":
    env = TreasureHuntEnv()
    pass
